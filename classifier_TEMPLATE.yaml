use_opt_flow: False

# Output debug information e.g. heat numbers on preview
debug: False

# root folder where the source and output folders belong
base_data_folder: "<full path>"
# Enables/Disables GPU accelerated classification
use_gpu: false

# source_folder is where the cptv files reside under. It is relative
# to base_data_folder.
source_folder: "cptv"

# tracks_folder is where the track data files will be created. It is
# specified relative to base_data_folder.
tracks_folder: "tracks"

# which folders (tags) to ignore when selecting cptv files from source_folder
excluded_tags:
  [
    "untagged",
    "unidentified",
    "hard",
    "multi",
    "moving",
    "mouse",
    "bird-kiwi",
    "for_grant",
    "missclassified",
    "other",
  ]

# Reprocess files which have already been processed.
reprocess: true

# Colour map override to use when exporting to MPEG. If not specified,
# a sensible default colour map is used.
# Note: This is should be a full path. It is not relative to the # base_data_folder.
# previews_colour_map: "custom_colormap.dat"

# Labels
labels : ['human','bird', 'cat', 'false-positive', 'hedgehog', 'insect', 'leporidae', 'mustelid', 'possum', 'rodent', 'wallaby']


# Number of worker threads to use.  0 disables worker pool and forces a single thread.
worker_threads: 0
# x and y resolution of thermal camera
res_x: 160
res_y: 120

tracking:
    #
    #  Tracking algorithm
    #

    # Default algorithm used to calculate the image background.   Must be "preview" or "stats"
    # Preview uses the preview time in the video (if it exists in the meta for cptv file)
    # Stats uses a statistical analysis of the whole video to get background levels
    background_calc: preview

    # When calculating the background ignore the last frames of the preview as the motion detection
    # will still be triggering during this time.
    preview_ignore_frames: 2

    motion:
        #if set to True Temp_thresh is calculated based on the minimum background temperature
        #of the preview, or whole clip depending on cptv type
        dynamic_thresh: True

        camera_thresholds:
          - camera_model: "lepton3"
            # Default temperature threshold require for tracking
            temp_thresh: 2900
            background_thresh: 20 # 1 degrees

            # Minimum raw temperature difference between background and track
            delta_thresh: 20
            # Min/Max temperature threshold value to use if dynamic threshold is true
            max_temp_thresh: null
            min_temp_thresh: null
            default: True
          # discard tracks that do not have enough delta within the window (i.e. pixels that change a lot)
            track_min_delta: 1.0
            track_max_delta: 150
          - camera_model: "lepton3.5"
            temp_thresh: 28000
            background_thresh: 90 # .9 degrees
            delta_thresh: 90 # .9 degrees
            max_temp_thresh: null
            min_temp_thresh: null
            # discard tracks that do not have enough delta within the window (i.e. pixels that change a lot)
            track_min_delta: 1.0
            track_max_delta: 150
    # these parameters are associated with the stats background method
    stats:
        # auto threshold needs to find a near maximum value to calculate the threshold level
        # a better solution might be the mean of the max of each frame?
        threshold_percentile: 99.9

        # minimum allowed threshold for number of pixels changed from average background, smaller values detect more objects, but bring up additional false positives
        min_threshold: 30

        # minimum allowed threshold for number of pixels changed from average background, smaller values detect more objects, but bring up additional false positives
        max_threshold: 50


    # any clips with a mean temperature hotter than this will be excluded
    max_mean_temperature_threshold: 10000

    # any clips with a temperature dynamic range greater than this will be excluded
    max_temperature_range_threshold: 10000

    # measures how different pixels are from estimated background, if they are on average less different than this then
    # video is considered to be static.
    static_background_threshold: 4.0

    # number of pixels round the edge to ignore due to strange values
    edge_pixels: 1

    # number of pixels around object to pad.
    # Note: frame_padding must be at least 3 or some frames may be too small for classification and program may crash
    frame_padding: 4

    # dilation pixels:  This is the number of pixels to grow the mask of interesting bits.
    # The higher the value, the further apart things (bits of animal, or animals) will be linked together as one object
    dilation_pixels: 2

    # wait this many frames for animal to "reappear" after it is last detected.
    remove_track_after_frames: 18

    # when enabled smooths tracks so that track dimensions do not change too quickly.
    track_smoothing: False

    high_quality_optical_flow: False

    # how much to threshold thermal before calculating optical flow.
    flow_threshold: 40

    # maximum number of tracks to extract from a clip.  Takes the n best tracks.
    max_tracks: 10

    areas_of_interest:
        # minimum pixels of interest each area of interest should have.
        min_mass: 4.0

        # minimum variation of pixels between then frame and previous for each area of interest.
        pixel_variance: 2.0

        # strategy to use when dealing with regions of interest that are cropped against the side of the frame
        # in general these regions often do not have enough information to accurately identify the animal.
        # options are
        # 'all': All cropped regions are included, good for classifier
        # 'cautious': Regions that are only cropped a bit are let through, this is good for training data
        # 'none': No cropped regions are permitted.  This is the most safe.
        cropped_regions_strategy: "cautious"

    filters:
        # regions with a movement less than this have their tracking scored based of x,y matching, instead of than mid points
        moving_vel_thresh: 4

        # discard any tracks that overlap with other tracks more than this.   T than this length in seconds
        track_overlap_ratio: 0.5

        # discard any tracks shorter than this length in seconds
        min_duration_secs: 1.0

        # dicard any tracks that do not move enough, (move less than this)
        track_min_offset: 4.0

      # discard tracks that do not have enough delta within the window (i.e. pixels that change a lot)
      track_min_delta: 1.0

      # discard tracks that do not have enough enough average mass.
      track_min_mass: 2.0

    # Add verbose logging about tracks generated
    verbose: False

    # Minimum confidence of track tag to accept
    min_tag_confidence: 0.8
load:
    # precidence of tags (lower first)
    tag_precedence:
        0: ["bird", "false-positive", "hedgehog", "possum", "rodent", "mustelid", "cat", "kiwi", "dog", "leporidae", "human", "insect", "pest"]
        1: ["unidentified", "other"]
        2: ["part","bad track"]

    # Use fast BLOSC compression (requires plugin), when saving tracks to database
    enable_compression: False

  # Includes the filtered channel in tracks database.  This is typically not used.  If compression is enabled the
  # filesize can be reduced by not including it.
  include_filtered_channel: True

  # Create a MP4 preview of the recording.  Options are "none", "raw", "boxes", "classified", "tracking"
  # none - won't create a preview video
  # raw - just the video (no classification or tracks)
  # boxes - show track boxes but no text
  # classified - show tracks and classification values
  # tracking -  four frame video view, including thermal, filtered, mask and flow layers
  preview: "none"
  high_quality_optical_flow: True

    #cache buffer frame to disk reducing memory usage
    cache_to_disk: False
train:
  # model_resnet, model_lq, or model_hq
  model: "keras"
  hyper_params:
    # inception rv3 do last 2 blocks > mixed_8_concatenate
    #retrain_layer: 249
    # inception resnet v2 do last 2 blocks > block_8_8_mixed_concatenate
    # layer 742 block8_8_mixed last 2 blocks
    # retrain_layer: 742
    # layer 758 block8_9_mixed  last 1 blocks
    #retrain_layer: 758
    model: "inceptionresnetv2"
    buffer_size: 4
    dropout: 0.1
    train_load_threads: 1
    # training
    learning_rate: 0.001
    shuffle: True
    # Number of epochs to train for
    epochs: 10

    # use a gru cell or a lstm cell
    use_gru: false

    # Location to write various training outputs to. Relative to
    # base_data_folder. Defaults to "training"
    # train_dir: "training"

classify_tracking:
    # Note: frame_padding must be at least 3 or some frames may be too small for classification
    frame_padding: 4
    high_quality_optical_flow: True
    filters:
        track_overlap_ratio: 0.5
        min_duration_secs: 1.0
        track_min_offset: 4.0
        track_min_delta: 1.0
        track_min_mass: 2.0

classify:
    # Writes metadata to standard out instead of a file with extension .txt
    meta_to_stdout : False

    # Path to pretrained Model use for classification (This should be the base filename without any extension)
    model: "<full path>"
    # Create a MP4 preview after classification of recording.  Options are "none", "raw", "classified", "tracking"
    # See extract:preview for details on each option.
    preview: "boxes"

    # folder is where classifier output and mp4s will be created.  It is specified relative to the base_data_folder
    classify_folder: "classify"

    #cache buffer frame to disk reducing memory usage
    cache_to_disk: False
evaluate:
    # Evalulates results against pre-tagged ground truth.
    show_extended_evaluation: False

    # number of seconds between clips required to trigger a a new visit
    new_visit_threshold: 180

    # false positive's and 'none' can be mapped to the same label as they represent the same idea.
    null_tags: ["false-positive", "none", "no-tag"]

build:
    # uses split from previous run
    use_previous_split: False

    # file to load previous split from
    previous_split: template.dat

    # labels to ignore
    ignore_labels: ["false-positive"]

    # if true removes any trapped animal footage from dataset.
    # trapped footage can be a problem as there tends to be lots of it and the animals do not move in a normal way.
    # however, bin weighting will generally take care of the excessive footage problem.
    excluded_trap: True

    # sets a maximum number of segments per bin, where the cap is this many standard deviations above the norm.
    # bins with more than this number of segments will be weighted lower so that their segments are not lost, but
    # will be sampled less frequently.
    cap_bin_weight: 1.5

    # adjusts the weight for each animal class.  Setting this lower for animals that are less represented can help
    # with training, otherwise the few examples we have will be used excessively.  This also helps build a prior for
    # the class suggesting that the class is more or less likely.  For example bumping up the human weighting will cause
    # the classifier lean towards guessing human when it is not sure.

    # xxx this doesn't actually work, and should be removed.
    label_weights: {"bird-kiwi": 0.1}

    # clips after this date will be ignored.
    # note: this is based on the UTC date.
    clip_end_date: None

    # minimum average mass for test segment
    test_min_mass: 20

    train_min_mass: 20

    # any day with a track this number of second or longer will be excluded from the validation set.
    # this is because it would be more useful to train on the long track, and we don't want the track to dominate the
    # validation set (otherwise a single track could end up being 50% of the data)
    max_validation_set_track_duration: 120

    # number of segments to include in test set for each class (multiplied by label weights)
    test_set_count: 300

    # minimum number of bins used for test set
    test_set_bins: 10

    # number of seconds each segment should be
    segment_length: 3

    # number of seconds segments are spaced apart
    segment_spacing: 1
